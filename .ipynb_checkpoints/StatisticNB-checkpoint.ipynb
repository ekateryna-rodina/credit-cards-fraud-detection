{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([4,5,1,2,7,2,6,9,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 4.0\n"
     ]
    }
   ],
   "source": [
    "dt_mean = np.mean(data)\n",
    "print(f'Mean: {np.floor(dt_mean)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median: 4.0\n"
     ]
    }
   ],
   "source": [
    "dt_median = np.median(data)\n",
    "print(f'Median: {dt_median}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: 2\n"
     ]
    }
   ],
   "source": [
    "dt_mode = stats.mode(data)\n",
    "print(f'Mode: {dt_mode[0][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diapertion - varitation of the data\n",
    "Range - diff min/max\n",
    "Variance - is a mean of squared deviations frrom the mean - population variance (/N), sample variance(/N-1) - unbiased - higher variation which is closer to population variance\n",
    "Standart deviation - square root of variance\n",
    "\n",
    "\n",
    "Quantiles: These are simply identical fragments of the data. Quantiles coverpercentiles, deciles, quartiles, and so on. These measures are calculated afterarranging the data in ascending order:Percentile: This is nothing but the percentage of data points belowthe value of the original whole data. The median is the 50thpercentile, as the number of data points below the median is about50 percent of the data.Decile: This is 10th percentile, which means the number of datapoints below the decile is 10 percent of the whole data.Quartile: This is one-fourth of the data, and also is the 25thpercentile. The first quartile is 25 percent of the data, the second quartile is 50 percent of the data, the third quartile is 75 percent ofthe data. The second quartile is also known as the median or 50thpercentile or 5th decile.Interquartile range: This is the difference between the thirdquartile and first quartile. It is effective in identifying outliers indata. The interquartile range describes the middle 50 percent of thedata points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import variance, stdev "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_points = np.array([35,56,43,59,63,79,35,41,64,43,93,60,77,24,82])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance: 400\n"
     ]
    }
   ],
   "source": [
    "# calculate variance\n",
    "dt_variance = variance(game_points); print(f'Variance: {round(dt_variance, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standart deviation: 20.0\n"
     ]
    }
   ],
   "source": [
    "# calculate standart deviation\n",
    "dt_st_dev = stdev(game_points); print(f'Standart deviation: {round(dt_st_dev, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range: 69\n"
     ]
    }
   ],
   "source": [
    "# calculate range\n",
    "dt_range = np.max(game_points, axis=0) - np.min(game_points, axis=0); print(f'Range: {dt_range}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20% 39.800000000000004\n",
      "80% 77.4\n",
      "100% 93.0\n"
     ]
    }
   ],
   "source": [
    "# calculate percentiles\n",
    "for i in (20, 80, 100):\n",
    "    perc = np.percentile(game_points, i)\n",
    "    print (str(i)+\"%\" ,perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQR: 28.5\n"
     ]
    }
   ],
   "source": [
    "# calculate IQR\n",
    "q1, q3 = np.percentile(game_points, [25, 75]); print(f'IQR: {q3-q1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis testing: This is the process of making inferences about the overallpopulation by conducting some statistical tests on a sample. Null and alternate hypotheses are ways to validate whether an assumption is statistically significantor not.\n",
    "\n",
    "P-value: The probability of obtaining a test statistic result is at least as extreme asthe one that was actually observed, assuming that the null hypothesis is true(usually in modeling, against each independent variable, a p-value less than 0.05is considered significant and greater than 0.05 is considered insignificant;nonetheless, these values and definitions may change with respect to context).\n",
    "\n",
    "The steps involved in hypothesis testing are as follows:Assume a null hypothesis (usually no difference, no significance, and1.so on; a null hypothesis always tries to assume that there is no anomalypattern and is always homogeneous, and so on).Collect the sample.2.Calculate test statistics from the sample in order to verify whether the3.hypothesis is statistically significant or not.Decide either to accept or reject the null hypothesis based on the test4.statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate test statistics\n",
    "from scipy import stats\n",
    "x_bar, mu0, std, n = 990, 1000, 12.5, 30 #measure, hypothesis, standart deviation, number of training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Stats: -4.38\n"
     ]
    }
   ],
   "source": [
    "# calculate test statistics\n",
    "test_stats = (x_bar - mu0)/(std/np.sqrt(n)); print(f'Test Stats: {round(test_stats, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical value from t table: -1.7\n"
     ]
    }
   ],
   "source": [
    "# critical value from t-table\n",
    "t_alpha = 0.05\n",
    "crit_val = stats.t.ppf(t_alpha, n-1); print(f'Critical value from t table: {round(crit_val, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowertail p-value from t-table 7.035025729010886e-05\n"
     ]
    }
   ],
   "source": [
    "# lower tail p_value from t ttable\n",
    "p_val = stats.t.sf(np.abs(test_stats), n-1); print (\"Lowertail p-value from t-table\", p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type I error: Rejecting a null hypothesis when it is trueType II error: Accepting a null hypothesis when it is false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal ditstribution - centralimit theorem\n",
    "from scipy import stats\n",
    "\n",
    "x, mu0, std = 67, 52, 16.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.920245398773006"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate z-score - how many standart deviations above or below population mean the score is\n",
    "z = (x - mu0)/std; z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability to score more then 67 is 17.9%\n"
     ]
    }
   ],
   "source": [
    "# calculating the probability under the curve\n",
    "p = 1 - stats.norm.cdf(z); print(f'Probability to score more then {x} is {round(p, 3)*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chi-square represents statistical dependence between categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# survey_tab = pd.crosstab(survey.Smoke, survey.Exer, margins= True)\n",
    "# Creating observed table for analysis>>> observed = survey_tab.ix[0:4,0:3]\n",
    "# contg = stats.chi2_contingency(observed= observed)\n",
    "# p_value = round(contg[1],3)\n",
    "# print (\"P-value is: \",p_value)\n",
    "#  Ifp-value < 0.05, there is a strong dependency between two variables, whereas ifp-value > 0.05, \n",
    "# there is no dependency between the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA: Analyzing variance tests the hypothesis that the means of two or morepopulations are equal. \n",
    "# one_way_anova = stats.f_oneway(fetilizers[\"fertilizer1\"],fetilizers[\"fertilizer2\"], fetilizers[\"fertilizer3\"])\n",
    "# print (\"Statistic :\", round(one_way_anova[0],2),\", p-value:\",round(one_way_anova[1],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision (P): When yes is predicted, how often is it correct?(TP/TP+FP)\n",
    "\n",
    "Recall (R)/sensitivity/true positive rate: Among the actual yeses, what fraction was predicted as yes?(TP/TP+FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score (F1): This is the harmonic mean of the precision and recall\n",
    "f1 = 2*P*R/(P+R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specificity: Among the actual nos, what fraction was predicted asno? Also equivalent to 1- false positive rate:(TN/TN+FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Area under curve (ROC): Receiver operating characteristic curve isused to plot between true positive rate (TPR) and false positiverate (FPR), also known as a sensitivity and 1- specificity graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusted R-squared - Adjusted R-squared value is the key metric in evaluating the quality of linearregressions. Any linear regression model having the value of R2 adjusted >=0.7 is considered as a good enough model to implement.\n",
    "\n",
    "Maximum likelihood estimate (MLE): This is estimating the parameter values ofa statistical model (logistic regression, to be precise) by finding the parametervalues that maximize the likelihood of making the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Akaike information criteria (AIC): This is used in logistic regression, which issimilar to the principle of adjusted R-square for linear regression. It measures therelative quality of a model for a given set of data. The idea of AIC is to penalize the objective function if extra variables withoutstrong predictive abilities are included in the model. This is a kind ofregularization in logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy: This comes from information theory and is the measure of impurity inthe data. If the sample is completely homogeneous, the entropy is zero and if thesample is equally divided, it has an entropy of 1. In decision trees, the predictorwith the most heterogeneousness will be considered nearest to the root node toclassify given data into classes in a greedy mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini: Gini impurity is a measure of misclassification, which applies in amulticlass classifier context. Gini works almost the same as entropy, except Giniis faster to calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning terminology for modelbuilding and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be an analogy between statistical modeling and machine learning that wewill cover in subsequent chapters in depth. However, a quick view has been provided asfollows: in statistical modeling, linear regression with two independent variables is tryingto fit the best plane with the least errors, whereas in machine learning independentvariables have been converted into the square of error terms (squaring ensures the functionwill become convex, which enhances faster convergence and also ensures a globaloptimum) and optimized based on coefficient values rather than independent variables.\n",
    "\n",
    "Convex functions are functions in which a line drawn between any tworandom points on the function also lies within the function, whereas this isn't true for non-convex functions. It is important to know whether the function is convex or non-convex dueto the fact that in convex functions, the local optimum is also the global optimum, whereasfor non-convex functions, the local optimum does not guarantee the global optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization techniques.\n",
    "\n",
    " * Gradient descent: This is a way to minimize the objective function J(Θ)parameterized by the model's parameter Θε Rd by updating the parameters inthe opposite direction to the gradient of the objective function with respect to theparameters. The learning rate determines the size of steps taken to reach theminimum.\n",
    " * Full batch gradient descent (all training observations considered in each andevery iteration): In full batch gradient descent, all the observations areconsidered for each and every iteration; this methodology takes a lot of memoryand will be slow as well. Also, in practice, we do not need to have all theobservations to update the weights. Nonetheless, this method provides the bestway of updating parameters with less noise at the expense of huge computation.\n",
    " * Stochastic gradient descent (one observation per iteration): This methodupdates weights by taking one observation at each stage of iteration. This methodprovides the quickest way of traversing weights; however, a lot of noise isinvolved while converging.\n",
    " * Mini batch gradient descent (about 30 training observations or more for eachand every iteration): This is a trade-off between huge computational costs and aquick method of updating weights. In this method, at each iteration, about 30observations will be selected at random and gradients calculated to update themodel weights. Here, a question many can ask is, why the minimum 30 and notany other number? If we look into statistical basics, 30 observations required tobe considering in order approximating sample as a population. However, even40, 50, and so on will also do well in batch size selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/mtcars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>mpg</th>\n",
       "      <th>cyl</th>\n",
       "      <th>disp</th>\n",
       "      <th>hp</th>\n",
       "      <th>drat</th>\n",
       "      <th>wt</th>\n",
       "      <th>qsec</th>\n",
       "      <th>vs</th>\n",
       "      <th>am</th>\n",
       "      <th>gear</th>\n",
       "      <th>carb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mazda RX4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.620</td>\n",
       "      <td>16.46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mazda RX4 Wag</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.875</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Datsun 710</td>\n",
       "      <td>22.8</td>\n",
       "      <td>4</td>\n",
       "      <td>108.0</td>\n",
       "      <td>93</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.320</td>\n",
       "      <td>18.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hornet 4 Drive</td>\n",
       "      <td>21.4</td>\n",
       "      <td>6</td>\n",
       "      <td>258.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.215</td>\n",
       "      <td>19.44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hornet Sportabout</td>\n",
       "      <td>18.7</td>\n",
       "      <td>8</td>\n",
       "      <td>360.0</td>\n",
       "      <td>175</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.440</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0   mpg  cyl   disp   hp  drat     wt   qsec  vs  am  gear  \\\n",
       "0          Mazda RX4  21.0    6  160.0  110  3.90  2.620  16.46   0   1     4   \n",
       "1      Mazda RX4 Wag  21.0    6  160.0  110  3.90  2.875  17.02   0   1     4   \n",
       "2         Datsun 710  22.8    4  108.0   93  3.85  2.320  18.61   1   1     4   \n",
       "3     Hornet 4 Drive  21.4    6  258.0  110  3.08  3.215  19.44   1   0     3   \n",
       "4  Hornet Sportabout  18.7    8  360.0  175  3.15  3.440  17.02   0   0     3   \n",
       "\n",
       "   carb  \n",
       "0     4  \n",
       "1     4  \n",
       "2     1  \n",
       "3     1  \n",
       "4     2  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to np to proceed scikit-learn\n",
    "X = np.array(train_data[\"hp\"])  ; y = np.array(train_data[\"mpg\"])\n",
    "X = X.reshape(32,1); y = y.reshape(32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept 30.098860539622496 Coefficient [-0.06822828]\n"
     ]
    }
   ],
   "source": [
    "# import Linear Regression - least squares method\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X, y)\n",
    "print (\"Intercept\",model.intercept_[0] ,\"Coefficient\", model.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent function\n",
    "def gradient_descent(x, y,learn_rate,conv_threshold,batch_size, max_iter):\n",
    "    converged=False\n",
    "    iter_=0\n",
    "#     init random weights\n",
    "    t0 = np.random.random(x.shape[1])\n",
    "    t1 = np.random.random(x.shape[1])\n",
    "    \n",
    "#     setup initial error\n",
    "    MSE = (sum([(t0 + t1*x[i] - y[i])**2 for i in range(batch_size)])/ batch_size)\n",
    "    while not converged:\n",
    "        w0 = 1.0/batch_size * sum([(t0 + t1*x[i] - y[i]) for i in range(batch_size)])\n",
    "        w1 = 1.0/batch_size * sum([(t0 + t1*x[i] - y[i])*x[i] for i in range(batch_size)])\n",
    "        \n",
    "        temp0 = t0 - learn_rate*w0\n",
    "        temp1 = t1 - learn_rate*w1\n",
    "        \n",
    "        t0 = temp0\n",
    "        t1 = temp1\n",
    "#         Calculate a new error with updated parameters\n",
    "        MSE_n = (sum( [ (t0 + t1*x[i] - y[i])**2 for i in range(batch_size)]) / batch_size)\n",
    "        \n",
    "        if abs(MSE_n - MSE) <= conv_threshold:\n",
    "            print(f'Convergence is riched on {iter_} iteration')\n",
    "            converged = True\n",
    "        MSE = MSE_n\n",
    "        iter_ += 1\n",
    "        if iter_ == max_iter:\n",
    "            print('Max iteration is riched')\n",
    "            converged = True\n",
    "    return t0, t1, MSE, iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence is riched on 1146600 iteration\n",
      "GD returnet optimal parameters with intercept at [30.02495114] and coefficientt at [-0.06781243]\n",
      "Intercept = [30.02495114] Coefficient = [-0.06781243]\n",
      "[13.99077647] 1146601\n"
     ]
    }
   ],
   "source": [
    "intercept, coeff, error, iter_ = gradient_descent(x = X,y = y,learn_rate=0.00003 ,conv_threshold = 1e-8, batch_size=32,max_iter=1500000)\n",
    "print(f'GD returnet optimal parameters with intercept at {intercept} and coefficientt at {coeff}')\n",
    "print (('Intercept = %s Coefficient = %s') %(intercept, coeff))\n",
    "print(error, iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
